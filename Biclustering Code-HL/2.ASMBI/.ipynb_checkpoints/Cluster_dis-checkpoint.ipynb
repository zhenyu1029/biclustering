{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "from math import log\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_matrix(labels_true, labels_pred):\n",
    "   \n",
    "\n",
    "    classes, class_idx = np.unique(labels_true, return_inverse=True)\n",
    "    clusters, cluster_idx = np.unique(labels_pred, return_inverse=True)\n",
    "    n_classes = classes.shape[0]\n",
    "    n_clusters = clusters.shape[0]\n",
    "    \n",
    "    contingency = sp.sparse.coo_matrix((np.ones(class_idx.shape[0]),\n",
    "                                 (class_idx, cluster_idx)),\n",
    "                                shape=(n_classes, n_clusters),\n",
    "                                dtype=np.int)\n",
    "\n",
    "    contingency = contingency.toarray()\n",
    " \n",
    "    return contingency\n",
    "\n",
    "\n",
    "def joint_entropy(labels_true, labels_pred):\n",
    "\n",
    "    contingency = contingency_matrix(labels_true, labels_pred)\n",
    "    nzx, nzy = np.nonzero(contingency)\n",
    "    nz_val = contingency[nzx, nzy]\n",
    "    contingency_sum = contingency.sum()\n",
    "\n",
    "    contingency_sum = contingency.sum()\n",
    "    log_contingency_nm = np.log(nz_val)\n",
    "    nz_val = map(float,nz_val)\n",
    "    contingency_nm = nz_val / contingency_sum\n",
    "    mi = - contingency_nm * (log_contingency_nm - log(contingency_sum))    \n",
    "    return mi.sum()\n",
    "\n",
    "def MI_distance(labels_true, labels_pred):\n",
    "    MI = mutual_info_score(labels_true, labels_pred)\n",
    "    H_xy = joint_entropy(labels_true, labels_pred)\n",
    "    return (H_xy- MI)/H_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Largest_dis_based_MI(seed,stocks,data):\n",
    "    Largest_dis = - np.inf\n",
    "    Largest_dis_stock = ''\n",
    "    for s in stocks:\n",
    "        s_iter = data.loc[s]\n",
    "        s_seed = data.loc[seed]\n",
    "        dis = MI_distance(s_seed,s_iter)\n",
    "        if Largest_dis < dis:\n",
    "            Largest_dis = dis\n",
    "            Largest_dis_stock = s\n",
    "    return Largest_dis_stock, Largest_dis\n",
    "\n",
    "def Creat_bicluster(n):\n",
    "    Bicluster = []\n",
    "    Bicluster_scores = []\n",
    "    for i in range(1,n+1):\n",
    "        Bicluster.append(['B_'+str(i)])\n",
    "        Bicluster_scores.append(['B_scores'+str(i)])\n",
    "    return Bicluster, Bicluster_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_pairs(data):\n",
    "    stocks = list(data.KEY.unique())\n",
    "    all_comb = list(itertools.combinations(stocks, 2))\n",
    "    return all_comb\n",
    "\n",
    "def Compute_All_MI(data):\n",
    "    Dis_pairs = []\n",
    "    pairs = Create_pairs(data)\n",
    "    data_with_index = data.set_index('KEY')\n",
    "    for e in pairs:\n",
    "        dis = MI_distance(data_with_index.loc[e[0]],data_with_index.loc[e[1]])\n",
    "        Dis_pairs.append(dis)\n",
    "    \n",
    "    S1 = []\n",
    "    S2 = []\n",
    "    for i in range(len(pairs)):\n",
    "        S1.append(pairs[i][0])\n",
    "        S2.append(pairs[i][1])\n",
    "        \n",
    "    Dis_pairs_df = pd.DataFrame([S1, S2, Dis_pairs])\n",
    "    Dis_pairs_df = Dis_pairs_df.T\n",
    "    Dis_pairs_df.columns=['S1', 'S2', 'Score']\n",
    "    #Dis_pairs_df.to_csv('MI_pairs_score.csv',index=False)\n",
    "    Sorted = Dis_pairs_df.sort_values(by=['Score'],ascending=False)\n",
    "    Largest_record = Sorted[:1]\n",
    "    seed1 = Largest_record.S1\n",
    "    seed2 = Largest_record.S2\n",
    "    return seed1.tolist()[0], seed2.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initial_seed(data, n):\n",
    "    #print n\n",
    "    Bicluster, Bicluster_scores = Creat_bicluster(n)\n",
    "    seed0, seed1 = Compute_All_MI(data)\n",
    "    seed_list = []\n",
    "    Bicluster[0].append(seed0)\n",
    "    Bicluster[1].append(seed1)\n",
    "    seed_list.append(seed0)\n",
    "    seed_list.append(seed1)\n",
    "    \n",
    "    stocks = list(data.KEY.unique())\n",
    "    data_with_index = data.set_index('KEY')\n",
    "    stocks_bucket = list(set(stocks)-set(seed_list))\n",
    "    #print 'initial stockbucket', stocks_bucket\n",
    "    for i in range(2, n):\n",
    "       # print i\n",
    "        largest_dis_stock=''\n",
    "        largest_dis = - np.inf\n",
    "        #print 'seed list', seed_list\n",
    "        \n",
    "        for stock in stocks_bucket:\n",
    "            dis_total = 0\n",
    "            j = 0\n",
    "            for seed in seed_list:\n",
    "               # print 'seed', seed\n",
    "                s_iter = data_with_index.loc[stock]\n",
    "                s_seed = data_with_index.loc[seed]\n",
    "                dis = MI_distance(s_seed,s_iter)\n",
    "                dis_total += dis\n",
    "                j += 1\n",
    "            Average_dis = dis_total*1./j\n",
    "        \n",
    "           # print 'average_MI_score',stock, Average_MI_score\n",
    "            if largest_dis < Average_dis:\n",
    "                largest_dis = Average_dis\n",
    "                largest_dis_stock = stock\n",
    "                #print 'lowest_score', lowest_score\n",
    "                #print 'lowest_score_stock', lowest_score_stock\n",
    "        #print 'lowest_score_stock', lowest_score_stock\n",
    "        #print 'lowest_score', lowest_score\n",
    "        stocks_bucket.remove(largest_dis_stock)\n",
    "        #print stocks_bucket\n",
    "        seed_list.append(largest_dis_stock)\n",
    "        Bicluster[i].append(largest_dis_stock)\n",
    "    #print stocks_bucket\n",
    "    return stocks_bucket, Bicluster, Bicluster_scores, n \n",
    "\n",
    "def Smallest_dis_MI(data, n):\n",
    "    \n",
    "    stocks_bucket, Bicluster, Bicluster_scores, n = Initial_seed(data, n)\n",
    "    data_with_index = data.set_index('KEY')\n",
    "    \n",
    "    for s in stocks_bucket:\n",
    "        \n",
    "        s_iter = data_with_index.loc[s]\n",
    "        \n",
    "        Smallest_dis_seed = ''\n",
    "        Smallest_dis = np.inf\n",
    "        \n",
    "        for i in range(0,n):\n",
    "            \n",
    "            Largest_dis = - np.inf\n",
    "            stock_in_bicluster = Bicluster[i][1:]\n",
    "            for s_b in stock_in_bicluster:\n",
    "                s_in_b = data_with_index.loc[s_b]\n",
    "                dis = MI_distance(s_in_b,s_iter)\n",
    "                if Largest_dis < dis:\n",
    "                    Largest_dis = dis\n",
    "            bicluster_MI = Largest_dis\n",
    "            if Smallest_dis > bicluster_MI:\n",
    "                Smallest_dis = bicluster_MI\n",
    "                Smallest_dis_seed_index = i\n",
    "                #print seed\n",
    "                #rint Highest_MI\n",
    "        Bicluster[Smallest_dis_seed_index].append(s)\n",
    "        Bicluster_scores[Smallest_dis_seed_index].append(str(round(Smallest_dis,3)))\n",
    "    return Bicluster, Bicluster_scores\n",
    "\n",
    "def Silhouette_score(data, n):\n",
    "    Bicluster, Bicluster_scores = Smallest_dis_MI(data, n)\n",
    "    data_with_index = data.set_index('KEY')\n",
    "    C = 0\n",
    "    Total_S_i = 0\n",
    "    for e in Bicluster:\n",
    "        #print e\n",
    "        a_i = 0\n",
    "        B_stocks = e[1:]\n",
    "        if len(B_stocks) == 1:\n",
    "            return 0\n",
    "        for ee in B_stocks:\n",
    "            #print ee\n",
    "            ee_data = data_with_index.loc[ee]\n",
    "            for i in range(len(B_stocks)):\n",
    "                if B_stocks[i] != ee:\n",
    "                    temp_data = data_with_index.loc[B_stocks[i]]\n",
    "                    a_temp = MI_distance(ee_data,temp_data)\n",
    "                    a_i += a_temp\n",
    "            \n",
    "            a_i /= (len(B_stocks)-1)\n",
    "            #print a_i\n",
    "           \n",
    "            \n",
    "            ## compute b_i\n",
    "            b_i =  np.inf\n",
    "            for e_other in Bicluster:\n",
    "                b_temp = 0\n",
    "                if e != e_other:\n",
    "                    B_other_stocks = e_other[1:]\n",
    "                    for ee_other in B_other_stocks:\n",
    "                        #print 'ee', ee_other\n",
    "                        ee_other_data = data_with_index.loc[ee_other]    \n",
    "                        other_temp = MI_distance(ee_other_data,ee_data)\n",
    "                        b_temp += other_temp\n",
    "                    b_temp_average = b_temp / (len(B_other_stocks))\n",
    "                    #print 'b_aver', b_temp_average\n",
    "                    if b_i >  b_temp_average:\n",
    "                        b_i = b_temp_average\n",
    "            S_i =  (b_i - a_i) / (max(b_i, a_i))\n",
    "            C += 1\n",
    "            Total_S_i += S_i\n",
    "    Average_S_i = Total_S_i / C\n",
    "    return Average_S_i\n",
    "     \n",
    "\n",
    "def determine_N(data):\n",
    "    Sil_score =[]\n",
    "    for n in range (2, 32):\n",
    "        temp = Silhouette_score(data, n)\n",
    "        Sil_score.append(temp)\n",
    "    \n",
    "    Max_value_index = Sil_score.index(max(Sil_score))\n",
    "    \n",
    "    # optimal number of cluster\n",
    "    n = Max_value_index + 2\n",
    "    \n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(data):\n",
    "    \n",
    "    n = determine_N(data)\n",
    "    print (\"optimal n: \",n)\n",
    "    Bicluster, Bicluster_scores = Smallest_dis_MI(data, n)\n",
    "    \n",
    "    return pd.DataFrame(Bicluster), len(Bicluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20130625\n",
      "('optimal n: ', 12)\n",
      "20130626\n",
      "('optimal n: ', 16)\n",
      "20130627\n",
      "('optimal n: ', 20)\n",
      "20130628\n",
      "('optimal n: ', 16)\n",
      "20130701\n",
      "('optimal n: ', 17)\n",
      "20130702\n",
      "('optimal n: ', 13)\n",
      "20130705\n",
      "('optimal n: ', 10)\n",
      "20130708\n",
      "('optimal n: ', 15)\n",
      "20130709\n",
      "('optimal n: ', 28)\n",
      "20130710\n",
      "('optimal n: ', 18)\n",
      "20130711\n",
      "('optimal n: ', 25)\n",
      "20130712\n",
      "('optimal n: ', 29)\n",
      "20130715\n",
      "('optimal n: ', 2)\n",
      "20130716\n",
      "('optimal n: ', 29)\n",
      "20130717\n",
      "('optimal n: ', 22)\n",
      "20130718\n",
      "('optimal n: ', 2)\n",
      "20130719\n",
      "('optimal n: ', 2)\n",
      "20130722\n",
      "('optimal n: ', 2)\n",
      "20130723\n",
      "('optimal n: ', 2)\n",
      "20130724\n",
      "('optimal n: ', 27)\n",
      "20130725\n",
      "('optimal n: ', 2)\n",
      "20130726\n",
      "('optimal n: ', 27)\n",
      "20130729\n",
      "('optimal n: ', 23)\n",
      "20130730\n",
      "('optimal n: ', 24)\n",
      "20130731\n",
      "('optimal n: ', 16)\n",
      "20130801\n",
      "('optimal n: ', 26)\n",
      "20130802\n",
      "('optimal n: ', 27)\n",
      "20130805\n",
      "('optimal n: ', 2)\n",
      "20130806\n",
      "('optimal n: ', 25)\n",
      "20130807\n",
      "('optimal n: ', 24)\n",
      "20130808\n",
      "('optimal n: ', 29)\n",
      "20130809\n",
      "('optimal n: ', 18)\n",
      "20130812\n",
      "('optimal n: ', 25)\n",
      "20130813\n",
      "('optimal n: ', 18)\n",
      "20130814\n",
      "('optimal n: ', 14)\n",
      "20130815\n",
      "('optimal n: ', 17)\n",
      "20130816\n",
      "('optimal n: ', 19)\n",
      "20130819\n",
      "('optimal n: ', 15)\n",
      "20130820\n",
      "('optimal n: ', 26)\n",
      "20130821\n",
      "('optimal n: ', 11)\n"
     ]
    }
   ],
   "source": [
    "datelist = pd.read_csv('./Quantile_10percent/stockdatelist.csv')\n",
    "date = datelist.date.tolist()\n",
    "date1 = date[120:160]   \n",
    "N_list = []\n",
    "for e in date1:\n",
    "    print (e)\n",
    "    #infile = '/Volumes/Haitao_SSD/2013sp100ticker_by_ticker/8Mutual_info/Quantile_10percent/stockreturn1min'+str(e)+'_digitized.csv'\n",
    "    #outfilepath = '/Volumes/Haitao_SSD/2013sp100ticker_by_ticker/8Mutual_info/Quantile_10percent_clusters_sil_dis/'\n",
    "    infile = './Quantile_10percent/stockreturn1min'+str(e)+'_digitized.csv'\n",
    "    outfilepath = './Quantile_10percent_clusters_sil_dis_120_160/'\n",
    "    \n",
    "    data = pd.read_csv(infile)\n",
    "    cluster, number_of_element = get_cluster(data)\n",
    "    cluster.to_csv(outfilepath+'clusters_'+str(e)+'.csv',index=False)\n",
    "    \n",
    "    N_list.append(number_of_element)\n",
    "    \n",
    "stats= pd.DataFrame([date1,N_list])\n",
    "stats = stats.transpose()\n",
    "stats.columns = ['date', 'N']\n",
    "stats.to_csv(outfilepath+'stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
